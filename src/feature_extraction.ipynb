{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import contextualSpellCheck\n",
    "import re\n",
    "import pickle\n",
    "import math\n",
    "import json\n",
    "from readability import Readability, exceptions\n",
    "from spacy_syllables import SpacySyllables\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe('syllables', after='tagger', config={\"lang\": \"en_US\"})\n",
    "\n",
    "contextualSpellCheck.add_to_pipe(nlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data  = json.loads(f.read())\n",
    "    return data\n",
    "\n",
    "emea = load_json(\"../data/EMEA.json\")\n",
    "gnome = load_json('../data/GNOME.json')\n",
    "jrc = load_json('../data/JRC.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "data_bio = data_set[data_set['topic']=='Biotech']\n",
    "data = data_bio[['id', 'text', 'date']]\n",
    "df = data[data.groupby('id')['id'].transform('size') > 50]\n",
    "print(len(df.groupby('id')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trigram_features(pos_string):\n",
    "    # Trigrams list\n",
    "    # NVN NNV VNN NADPPROPN NNCCONJ NOUNPREPDET DETNOUNPREP VNV NNN  NOUNPREPNOUN VPREPDET\n",
    "    features = {}\n",
    "    features['nvn'] = len(re.findall('(NOUN VERB NOUN)', pos_string))\n",
    "    features['nnv'] = len(re.findall('(NOUN NOUN VERB)', pos_string))\n",
    "    features['vnn'] = len(re.findall('(VERB NOUN NOUN)', pos_string))\n",
    "    features['vnv'] = len(re.findall('(VERB NOUN VERB)', pos_string))\n",
    "    features['nap'] = len(re.findall('(NOUN ADP PROPN)', pos_string))\n",
    "    features['nnc'] = len(re.findall('(NOUN NOUN CCONJ)', pos_string))\n",
    "    features['nad'] = len(re.findall('(NOUN ADP DET)', pos_string))\n",
    "    features['dna'] = len(re.findall('(DET NOUN ADP)', pos_string))\n",
    "    features['nnn'] = len(re.findall('(NOUN NOUN NOUN)', pos_string))\n",
    "    features['nan'] = len(re.findall('(NOUN ADP NOUN)', pos_string))\n",
    "    features['vad'] = len(re.findall('(VERB ADP DET)', pos_string))\n",
    "    \n",
    "    return features \n",
    "\n",
    "def rank_words(word_freq):\n",
    "    freq_sorted = dict(sorted(word_freq.items(), key=lambda item: item[1], reverse=True))\n",
    "    prev = max(freq_sorted.values())\n",
    "    rank = 1\n",
    "    ranks = {}\n",
    "    for word, freq in freq_sorted.items():\n",
    "        if freq < prev:\n",
    "            rank += 1\n",
    "            prev = freq\n",
    "        ranks[word] = rank\n",
    "    return ranks\n",
    "\n",
    "def get_average_word_rank(word_freq):\n",
    "    ranks = rank_words(word_freq) \n",
    "    return sum(list(ranks.values()))/len(ranks)\n",
    "\n",
    "def get_lemma_freq(lemma, word_freq):\n",
    "    if lemma in word_freq.keys():\n",
    "        return word_freq[lemma]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "closed_class = ['ADP','AUX','CCONJ','DET','NUM','PART','PRON','SCONJ']\n",
    "\n",
    "def get_yule_score(freq_dict):\n",
    "    m1 = sum(freq_dict.values())\n",
    "    m2 = sum([freq ** 2 for freq in freq_dict.values()])\n",
    "    if m2 > m1:\n",
    "        i = (m1*m1) / (m2-m1)\n",
    "    else:\n",
    "        i = -1\n",
    "    k = 10000/i\n",
    "    return k\n",
    "\n",
    "\n",
    "def get_honore(freq_dict, text_len):\n",
    "    hapax_legomena = list(freq_dict.values()).count(1)\n",
    "    if hapax_legomena == len(freq_dict):\n",
    "        return 0\n",
    "    return 100 * math.log(text_len)/(1-(hapax_legomena/len(freq_dict)))\n",
    "\n",
    "\n",
    "def get_features(df):\n",
    "    \n",
    "    id_tags = df['id'].unique()\n",
    "    ids = id_tags[:10]\n",
    "    all_features = pd.DataFrame(columns=['id','yule', 'fk_grade', 'f_reading', 'gunning_fog', 'honore_r', 'avg_word_length', 'syllable_no', 'spelling_errors', 'no_tag', 'sym', 'punct', 'mean_word_rank', 'of_freq', 'is_freq', 'the_freq', 'been_freq','nvn','nnv','vnn','vnv','nap','nnc','nad','dna','nnn','nan','vad'])\n",
    "    i = 0\n",
    "    for id in ids:\n",
    "        features_author = []\n",
    "        for text in df.loc[df['id']==id, 'text']:\n",
    "            \n",
    "            if i % 50 == 0:\n",
    "                print(\"Computed features of {} of texts\".format(i/len(df['text'])))\n",
    "            i += 1\n",
    "            \n",
    "            if len(text) > 1000000:\n",
    "                print(\"Text too long. Skipped.\")\n",
    "                continue\n",
    "            try:\n",
    "                tokenized = nlp(text)\n",
    "            except RuntimeError:\n",
    "                print(\"Runtime\")\n",
    "                continue\n",
    "    \n",
    "            pos_only = []\n",
    "            word_freq = {}\n",
    "            word_length = 0\n",
    "            syllable_no = 0\n",
    "            word_count = 0\n",
    "            \n",
    "            for token in tokenized:\n",
    "                if not token.text.isspace():\n",
    "                    pos_only.append(token.pos_)\n",
    "                    if token.pos_ not in ['X', 'SYM', 'PUNCT']:\n",
    "                        word_length += len(token.text)\n",
    "                        if token._.syllables_count:\n",
    "                            syllable_no += token._.syllables_count\n",
    "                        word_count += 1\n",
    "                    # print(token.lemma_, token.pos_, token.tag_)\n",
    "                    if token.lemma_ not in word_freq.keys():\n",
    "                        word_freq[token.lemma_] = 1\n",
    "                    else:\n",
    "                        word_freq[token.lemma_] += 1\n",
    "                        \n",
    "            if len(pos_only) < 1 or word_count < 1:\n",
    "                continue\n",
    "                    \n",
    "            features = get_trigram_features(' '.join(pos_only))\n",
    "            \n",
    "            features['id']=id\n",
    "            features['spelling_errors'] = len(tokenized._.suggestions_spellCheck)\n",
    "            features['no_tag'] = pos_only.count('X')\n",
    "            features['sym'] = pos_only.count('SYM')\n",
    "            features['punct'] = pos_only.count('PUNCT')\n",
    "            freq_sorted = dict(sorted(word_freq.items(), key=lambda item: item[1]))\n",
    "            \n",
    "            features['yule'] = get_yule_score(freq_sorted)\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                r = Readability(text)\n",
    "                features['fk_grade'] = r.flesch_kincaid().grade_level\n",
    "                features['f_reading'] = r.flesch().ease\n",
    "                features['gunning_fog'] = r.gunning_fog().score\n",
    "            except:\n",
    "                features['fk_grade'] = 0\n",
    "                features['f_reading'] = 0\n",
    "                features['gunning_fog'] = 0\n",
    "                \n",
    "            \n",
    "            features['honore_r'] = get_honore(freq_sorted, len(pos_only))\n",
    "            \n",
    "            features['mean_word_rank'] = get_average_word_rank(freq_sorted)\n",
    "            features['of_freq'] = get_lemma_freq('of', freq_sorted)\n",
    "            features['is_freq'] = get_lemma_freq('is', freq_sorted)\n",
    "            features['the_freq'] = get_lemma_freq('the', freq_sorted)\n",
    "            features['been_freq'] = get_lemma_freq('been', freq_sorted)\n",
    "\n",
    "            features['avg_word_length'] = word_length/word_count\n",
    "            features['sylablle_no'] = syllable_no/word_count\n",
    "            \n",
    "            features_author.append(features)\n",
    "        \n",
    "        features_df = pd.DataFrame(features_author)\n",
    "        all_features = all_features.append(features_df, ignore_index=True)\n",
    "        \n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed features of 0.0 of texts\n",
      "Computed features of 0.0324254215304799 of texts\n",
      "Computed features of 0.0648508430609598 of texts\n",
      "Computed features of 0.09727626459143969 of texts\n",
      "Computed features of 0.1297016861219196 of texts\n",
      "Computed features of 0.1621271076523995 of texts\n",
      "Computed features of 0.19455252918287938 of texts\n",
      "Runtime\n",
      "Computed features of 0.22697795071335927 of texts\n",
      "Computed features of 0.2594033722438392 of texts\n",
      "Computed features of 0.2918287937743191 of texts\n",
      "Computed features of 0.324254215304799 of texts\n",
      "Computed features of 0.35667963683527887 of texts\n",
      "Computed features of 0.38910505836575876 of texts\n",
      "Computed features of 0.42153047989623865 of texts\n",
      "Computed features of 0.45395590142671854 of texts\n",
      "Computed features of 0.48638132295719844 of texts\n",
      "Computed features of 0.5188067444876784 of texts\n",
      "Computed features of 0.5512321660181583 of texts\n",
      "Computed features of 0.5836575875486382 of texts\n",
      "Computed features of 0.6160830090791181 of texts\n",
      "Computed features of 0.648508430609598 of texts\n",
      "Computed features of 0.6809338521400778 of texts\n",
      "Computed features of 0.7133592736705577 of texts\n",
      "Computed features of 0.7457846952010376 of texts\n",
      "Computed features of 0.7782101167315175 of texts\n",
      "Computed features of 0.8106355382619974 of texts\n",
      "Computed features of 0.8430609597924773 of texts\n",
      "Computed features of 0.8754863813229572 of texts\n",
      "Computed features of 0.9079118028534371 of texts\n",
      "Computed features of 0.940337224383917 of texts\n",
      "Runtime\n",
      "Computed features of 0.9727626459143969 of texts\n"
     ]
    }
   ],
   "source": [
    "features = get_features(df)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('../output/features/features_biotech.pkl', 'wb') as file:\n",
    "    pickle.dump(features, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "INTJ\n",
      "1\n",
      "PRON\n",
      "None\n",
      "AUX\n",
      "1\n",
      "VERB\n",
      "3\n",
      "VERB\n",
      "1\n",
      "ADV\n",
      "3\n",
      "NOUN\n",
      "1\n",
      "PRON\n",
      "1\n",
      "VERB\n",
      "1\n",
      "SCONJ\n",
      "3\n",
      "PROPN\n",
      "1\n",
      "AUX\n",
      "1\n",
      "DET\n",
      "2\n",
      "NOUN\n",
      "1\n",
      "ADP\n",
      "2\n",
      "PROPN\n",
      "1\n",
      "ADP\n",
      "1\n",
      "DET\n",
      "1\n",
      "NOUN\n",
      "1\n",
      "ADP\n",
      "1\n",
      "PROPN\n",
      "1\n",
      "DET\n",
      "1\n",
      "VERB\n",
      "3\n",
      "PROPN\n",
      "1\n",
      "CCONJ\n",
      "1\n",
      "INTJ\n",
      "1\n",
      "ADV\n",
      "1\n",
      "PRON\n",
      "1\n",
      "VERB\n",
      "1\n",
      "ADJ\n",
      "1\n",
      "PRON\n",
      "1\n",
      "VERB\n",
      "1\n",
      "ADP\n",
      "1\n",
      "DET\n",
      "2\n",
      "ADV\n",
      "1\n",
      "VERB\n",
      "3\n",
      "PROPN\n",
      "1\n",
      "PROPN\n",
      "2\n",
      "NOUN\n",
      "1\n",
      "CCONJ\n",
      "1\n",
      "VERB\n",
      "1\n",
      "PROPN\n",
      "1\n",
      "ADP\n",
      "1\n",
      "DET\n",
      "1\n",
      "NOUN\n",
      "1\n",
      "NOUN\n",
      "1\n",
      "ADP\n",
      "1\n",
      "DET\n",
      "1\n",
      "ADJ\n",
      "2\n",
      "ADJ\n",
      "1\n",
      "NOUN\n",
      "1\n",
      "PRON\n",
      "1\n",
      "VERB\n",
      "1\n",
      "DET\n",
      "2\n",
      "NOUN\n",
      "2\n",
      "VERB\n",
      "3\n",
      "PROPN\n",
      "1\n",
      "ADP\n",
      "1\n",
      "PRON\n",
      "1\n",
      "NOUN\n",
      "1\n",
      "ADV\n",
      "1\n",
      "DET\n",
      "1\n",
      "NOUN\n",
      "1\n",
      "INTJ\n",
      "1\n",
      "NUM\n",
      "2\n",
      "NOUN\n",
      "2\n",
      "ADJ\n",
      "3\n",
      "ADJ\n",
      "1\n",
      "NOUN\n",
      "1\n",
      "VERB\n",
      "1\n",
      "CCONJ\n",
      "1\n",
      "VERB\n",
      "1\n",
      "SCONJ\n",
      "1\n",
      "PRON\n",
      "2\n",
      "ADV\n",
      "1\n",
      "ADV\n",
      "1\n",
      "PRON\n",
      "1\n",
      "VERB\n",
      "1\n",
      "ADP\n",
      "1\n",
      "DET\n",
      "1\n",
      "NOUN\n",
      "1\n",
      "PRON\n",
      "1\n",
      "VERB\n",
      "1\n",
      "SCONJ\n",
      "1\n",
      "DET\n",
      "2\n",
      "NOUN\n",
      "1\n",
      "ADV\n",
      "1\n",
      "ADV\n",
      "1\n",
      "ADP\n",
      "1\n",
      "PRON\n",
      "1\n",
      "CCONJ\n",
      "1\n",
      "ADP\n",
      "1\n",
      "DET\n",
      "1\n",
      "NOUN\n",
      "1\n",
      "AUX\n",
      "3\n",
      "PROPN\n",
      "1\n",
      "ADV\n",
      "1\n",
      "ADV\n",
      "1\n",
      "ADP\n",
      "1\n",
      "DET\n",
      "1\n",
      "NOUN\n",
      "1\n",
      "ADV\n",
      "1\n",
      "PRON\n",
      "1\n",
      "VERB\n",
      "1\n",
      "SCONJ\n",
      "3\n",
      "DET\n",
      "1\n",
      "NUM\n",
      "1\n",
      "NOUN\n",
      "1\n",
      "ADP\n",
      "2\n",
      "NOUN\n",
      "1\n",
      "AUX\n",
      "3\n",
      "ADJ\n",
      "1\n",
      "ADV\n",
      "1\n",
      "ADV\n",
      "1\n",
      "PRON\n",
      "None\n",
      "VERB\n",
      "3\n",
      "ADV\n",
      "1\n",
      "PRON\n",
      "None\n",
      "AUX\n",
      "4\n",
      "ADV\n",
      "2\n",
      "ADV\n",
      "1\n",
      "ADJ\n",
      "1\n",
      "NUM\n",
      "1\n",
      "NOUN\n",
      "1\n",
      "ADP\n",
      "3\n",
      "PROPN\n",
      "1\n",
      "AUX\n",
      "1\n",
      "PRON\n",
      "1\n",
      "VERB\n",
      "1\n",
      "DET\n",
      "2\n",
      "ADV\n",
      "1\n",
      "ADJ\n",
      "1\n",
      "NOUN\n",
      "1\n",
      "ADP\n",
      "4\n",
      "NOUN\n",
      "1\n",
      "VERB\n",
      "1\n",
      "DET\n",
      "1\n",
      "NOUN\n",
      "1\n",
      "ADP\n",
      "1\n",
      "ADJ\n",
      "2\n",
      "NOUN\n",
      "1\n",
      "PRON\n",
      "1\n",
      "VERB\n",
      "None\n",
      "PART\n",
      "2\n",
      "NOUN\n",
      "3\n",
      "ADV\n",
      "1\n",
      "CCONJ\n",
      "1\n",
      "PRON\n",
      "1\n",
      "VERB\n",
      "1\n",
      "PART\n",
      "1\n",
      "VERB\n",
      "2\n",
      "ADV\n",
      "1\n",
      "VERB\n",
      "2\n",
      "ADJ\n",
      "1\n",
      "CCONJ\n",
      "1\n",
      "ADP\n",
      "1\n",
      "ADJ\n",
      "1\n",
      "PRON\n",
      "1\n",
      "VERB\n",
      "1\n",
      "DET\n",
      "1\n",
      "NOUN\n",
      "1\n",
      "PART\n",
      "1\n",
      "VERB\n",
      "1\n",
      "ADP\n",
      "3\n",
      "PROPN\n",
      "1\n",
      "ADP\n",
      "1\n",
      "PRON\n",
      "1\n",
      "AUX\n",
      "1\n",
      "VERB\n",
      "1\n",
      "NOUN\n",
      "2\n",
      "VERB\n",
      "1\n",
      "ADP\n",
      "1\n",
      "DET\n",
      "2\n",
      "ADJ\n",
      "1\n",
      "ADP\n",
      "2\n",
      "NOUN\n",
      "1\n",
      "CCONJ\n",
      "2\n",
      "NOUN\n",
      "1\n",
      "ADV\n",
      "1\n",
      "ADV\n",
      "1\n",
      "ADP\n",
      "None\n",
      "NUM\n",
      "2\n",
      "NOUN\n",
      "1\n",
      "ADP\n",
      "2\n",
      "VERB\n",
      "1\n",
      "NUM\n",
      "3\n",
      "DET\n",
      "1\n",
      "ADP\n",
      "1\n",
      "PRON\n",
      "1\n",
      "AUX\n",
      "1\n",
      "VERB\n",
      "1\n",
      "DET\n",
      "1\n",
      "AUX\n",
      "1\n",
      "PRON\n",
      "1\n",
      "VERB\n",
      "1\n",
      "ADV\n",
      "5\n",
      "ADJ\n",
      "1\n",
      "ADP\n",
      "1\n",
      "ADJ\n",
      "2\n",
      "NOUN\n",
      "1\n",
      "ADP\n",
      "1\n",
      "PRON\n",
      "1\n",
      "NOUN\n",
      "1\n",
      "PRON\n",
      "1\n",
      "NOUN\n",
      "1\n",
      "VERB\n",
      "1\n",
      "DET\n",
      "1\n",
      "NOUN\n",
      "1\n",
      "ADP\n",
      "1\n",
      "DET\n",
      "1\n",
      "NOUN\n",
      "None\n",
      "PART\n",
      "1\n",
      "NOUN\n",
      "1\n",
      "CCONJ\n",
      "2\n",
      "ADJ\n",
      "2\n",
      "NOUN\n",
      "1\n",
      "ADP\n",
      "1\n",
      "VERB\n",
      "1\n",
      "ADP\n",
      "1\n",
      "DET\n",
      "1\n",
      "ADJ\n",
      "2\n",
      "NOUN\n",
      "1\n",
      "PRON\n",
      "1\n",
      "VERB\n",
      "1\n",
      "DET\n",
      "2\n",
      "ADJ\n",
      "1\n",
      "NOUN\n",
      "1\n",
      "ADP\n",
      "2\n",
      "DET\n",
      "3\n",
      "NOUN\n",
      "2\n",
      "NOUN\n",
      "2\n",
      "ADV\n",
      "1\n",
      "SCONJ\n",
      "3\n",
      "PROPN\n",
      "1\n",
      "AUX\n",
      "None\n",
      "PART\n",
      "3\n",
      "ADV\n",
      "1\n",
      "VERB\n",
      "1\n",
      "ADP\n",
      "2\n",
      "PROPN\n",
      "1\n",
      "DET\n",
      "1\n",
      "DET\n",
      "1\n",
      "ADV\n",
      "1\n",
      "CCONJ\n",
      "2\n",
      "ADV\n",
      "1\n",
      "PRON\n",
      "3\n",
      "ADV\n",
      "3\n",
      "ADV\n",
      "4\n",
      "VERB\n",
      "2\n",
      "VERB\n",
      "1\n",
      "ADP\n",
      "2\n",
      "PRON\n",
      "1\n",
      "ADP\n",
      "1\n",
      "NOUN\n",
      "1\n",
      "ADV\n",
      "1\n",
      "DET\n",
      "4\n",
      "NOUN\n",
      "1\n",
      "ADV\n",
      "1\n",
      "AUX\n",
      "1\n",
      "ADV\n",
      "1\n",
      "ADJ\n",
      "1\n",
      "ADV\n",
      "1\n",
      "PRON\n",
      "None\n",
      "AUX\n",
      "1\n",
      "ADV\n",
      "1\n",
      "VERB\n",
      "2\n",
      "NOUN\n",
      "2\n",
      "SCONJ\n",
      "1\n",
      "PRON\n",
      "None\n",
      "VERB\n",
      "2\n",
      "ADV\n",
      "1\n",
      "ADJ\n",
      "1\n",
      "PART\n",
      "1\n",
      "PART\n",
      "1\n",
      "VERB\n",
      "1\n",
      "PART\n",
      "1\n",
      "VERB\n",
      "1\n",
      "ADP\n",
      "1\n",
      "PRON\n",
      "1\n",
      "DET\n",
      "1\n",
      "NOUN\n",
      "1\n",
      "VERB\n",
      "1\n",
      "PART\n",
      "1\n",
      "VERB\n",
      "1\n",
      "PRON\n",
      "2\n",
      "VERB\n",
      "1\n",
      "DET\n",
      "1\n",
      "NOUN\n",
      "1\n",
      "ADV\n",
      "3\n",
      "ADV\n",
      "1\n",
      "PRON\n",
      "1\n",
      "VERB\n",
      "1\n",
      "ADP\n",
      "None\n",
      "NUM\n",
      "1\n",
      "CCONJ\n",
      "None\n",
      "NUM\n",
      "1\n",
      "ADV\n",
      "1\n",
      "VERB\n",
      "1\n",
      "ADP\n",
      "1\n",
      "ADP\n",
      "None\n",
      "NUM\n",
      "1\n",
      "CCONJ\n",
      "1\n",
      "PRON\n",
      "None\n",
      "VERB\n",
      "1\n",
      "PART\n",
      "1\n",
      "DET\n",
      "1\n",
      "ADJ\n",
      "1\n",
      "NOUN\n",
      "1\n",
      "NOUN\n",
      "1\n",
      "NOUN\n",
      "1\n",
      "PRON\n",
      "1\n",
      "ADJ\n",
      "1\n",
      "NOUN\n",
      "1\n",
      "VERB\n",
      "1\n",
      "PART\n",
      "1\n",
      "VERB\n",
      "None\n",
      "NUM\n",
      "None\n",
      "NUM\n",
      "1\n",
      "NOUN\n",
      "1\n",
      "NOUN\n",
      "1\n",
      "ADP\n",
      "None\n",
      "NUM\n",
      "1\n",
      "CCONJ\n",
      "None\n",
      "NUM\n",
      "1\n",
      "ADV\n",
      "1\n",
      "PRON\n",
      "1\n",
      "VERB\n",
      "1\n",
      "ADP\n",
      "1\n",
      "ADP\n",
      "1\n",
      "DET\n",
      "1\n",
      "PRON\n",
      "None\n",
      "AUX\n",
      "3\n",
      "ADV\n",
      "1\n",
      "ADP\n",
      "2\n",
      "VERB\n",
      "1\n",
      "ADP\n",
      "1\n",
      "ADP\n",
      "1\n",
      "DET\n",
      "2\n",
      "NOUN\n",
      "1\n",
      "CCONJ\n",
      "2\n",
      "VERB\n",
      "1\n",
      "ADP\n",
      "1\n",
      "ADP\n",
      "1\n",
      "DET\n",
      "2\n",
      "NOUN\n",
      "1\n",
      "VERB\n",
      "1\n",
      "ADP\n",
      "2\n",
      "VERB\n",
      "1\n",
      "ADP\n",
      "1\n",
      "ADP\n",
      "2\n",
      "NOUN\n",
      "1\n",
      "PRON\n",
      "2\n",
      "NOUN\n",
      "1\n",
      "ADP\n",
      "1\n",
      "NOUN\n",
      "1\n",
      "VERB\n",
      "1\n",
      "PRON\n",
      "1\n",
      "AUX\n",
      "1\n",
      "ADV\n",
      "2\n",
      "VERB\n",
      "None\n",
      "NUM\n",
      "None\n",
      "NOUN\n",
      "1\n",
      "ADP\n",
      "1\n",
      "INTJ\n",
      "None\n",
      "NUM\n",
      "1\n",
      "NOUN\n",
      "1\n",
      "CCONJ\n",
      "1\n",
      "PRON\n",
      "None\n",
      "VERB\n",
      "1\n",
      "PART\n",
      "1\n",
      "ADJ\n",
      "1\n",
      "SCONJ\n",
      "1\n",
      "PRON\n",
      "None\n",
      "AUX\n",
      "1\n",
      "VERB\n",
      "1\n",
      "DET\n",
      "1\n",
      "NOUN\n",
      "1\n",
      "SCONJ\n",
      "1\n",
      "PRON\n",
      "1\n",
      "VERB\n",
      "1\n",
      "PART\n",
      "1\n",
      "PRON\n",
      "1\n",
      "AUX\n",
      "1\n",
      "VERB\n",
      "1\n",
      "PRON\n",
      "1\n",
      "CCONJ\n",
      "1\n",
      "PRON\n",
      "None\n",
      "VERB\n",
      "1\n",
      "ADP\n",
      "1\n",
      "PRON\n",
      "None\n",
      "VERB\n",
      "1\n",
      "ADP\n",
      "3\n",
      "NOUN\n",
      "1\n",
      "ADV\n",
      "2\n",
      "ADV\n",
      "1\n",
      "PRON\n",
      "1\n",
      "VERB\n",
      "1\n",
      "PRON\n",
      "1\n",
      "NOUN\n",
      "1\n",
      "ADP\n",
      "1\n",
      "NOUN\n",
      "2\n",
      "ADV\n",
      "1\n",
      "ADV\n",
      "1\n",
      "DET\n",
      "1\n",
      "NOUN\n",
      "1\n",
      "VERB\n",
      "1\n",
      "ADJ\n",
      "1\n",
      "CCONJ\n",
      "1\n",
      "PROPN\n",
      "1\n",
      "VERB\n",
      "1\n",
      "ADJ\n",
      "1\n",
      "PRON\n",
      "1\n",
      "VERB\n",
      "1\n",
      "DET\n",
      "None\n",
      "AUX\n",
      "1\n",
      "PRON\n",
      "1\n",
      "ADP\n",
      "1\n",
      "ADV\n",
      "2\n",
      "PRON\n",
      "1\n",
      "ADV\n",
      "2\n",
      "ADJ\n",
      "1\n",
      "SCONJ\n",
      "1\n",
      "DET\n",
      "1\n",
      "ADP\n",
      "1\n",
      "DET\n",
      "1\n",
      "NOUN\n",
      "1\n",
      "DET\n",
      "1\n",
      "VERB\n",
      "1\n",
      "DET\n",
      "2\n",
      "ADJ\n",
      "1\n",
      "ADJ\n",
      "1\n",
      "PRON\n",
      "1\n",
      "VERB\n",
      "1\n",
      "PRON\n",
      "1\n",
      "ADV\n",
      "1\n",
      "ADV\n",
      "1\n",
      "ADJ\n",
      "2\n",
      "ADJ\n",
      "1\n",
      "CCONJ\n",
      "1\n",
      "ADJ\n",
      "1\n",
      "PART\n",
      "1\n",
      "VERB\n",
      "1\n",
      "CCONJ\n",
      "1\n",
      "PRON\n",
      "2\n",
      "ADV\n",
      "1\n",
      "VERB\n",
      "1\n",
      "PROPN\n",
      "1\n",
      "ADV\n",
      "1\n",
      "ADV\n",
      "1\n",
      "ADJ\n",
      "1\n",
      "ADJ\n",
      "1\n",
      "CCONJ\n",
      "1\n",
      "ADJ\n",
      "1\n",
      "PART\n",
      "1\n",
      "AUX\n",
      "1\n",
      "VERB\n",
      "1\n",
      "ADP\n",
      "1\n",
      "ADP\n",
      "1\n",
      "NOUN\n",
      "4\n",
      "NOUN\n",
      "1\n",
      "ADP\n",
      "4\n",
      "NOUN\n"
     ]
    }
   ],
   "source": [
    "text = \"Well, I'll stand corrected, again. Yesterday I blogged that Coquitlam was an oasis of Whiteness in the sea of Asians that is Vancouver, but oh how I was wrong.  We went to the, aptly named, Coquitlam Mall today and put Spencer in the play area with a few other kids.  I heard a lady speaking Korean to her boy, then a flock (well, three) ajumas (older Korean ladies) came and sat near us.  Later, when we went to the pool, we saw that the neighbors right next to us and across the street are Koreans too.  Then on the way back I learned that another two sets of neighbors are Korean as well.  They\\'re everywhere!  It\\'s actually pretty cool.  One thing about Koreans is they have a pretty good sense of community.  Get a group of white people (who aren\\'t farmers) together and we seem to be pretty stand-offish...or at least it takes a while to warm up.  Koreans, as you may know,  urlLink refer to each other as sister or brother  as soon as 3 minutes of meeting one another (as I have seen).  This is, I think, quite extraordinary.  From first meetings with my son my wife called the girls in the area \\'nuna\\' or \\'older sister, as said by a  younger brother\\' (they have a special word for every family member).  Also, since Koreans don\\'t normally take to English all that well (or quickly) they really, really appreciate speaking with someone from home.  Thus, the community here is quite tight.  So I\\'ve still got jetlag...although it\\'s pretty nice not to have to work through it.  The boys tend to keep me moving all day, though.  Usually I sleep at 3AM or 4AM then get up at 11AM, but it\\'s not a good  urlLink REM  sleep.  My best sleep seems to be 1-2 hour naps at 3PM or 7PM.  When I wake up from those it\\'s really like getting up in the morning; and getting up in the morning is like waking up at midnight.  My father-in-law thinks I should just adjust 100% in like 2 days, but I\\'m not sure if I\\'m built that way.  If I need to I can do it, but it\\'s like I\\'m on vacation here.  Besides, I do my work at night anyways, when the kids are asleep (and Seoul is awake).  I guess that\\'s it for now.  Nothing more profound than this from the day, which is a little sad.  I love it here: so clean, peaceful and easy to live; but I kinda miss Seoul too: so dirty, loud and easy to get run down by  urlLink deliveryboys on motorcycles.\"\n",
    "for w in nlp(text):\n",
    "    if not w.text.isspace() and w.pos_ not in ['X', 'SYM', 'PUNCT']:\n",
    "        print(w._.syllables_count)\n",
    "        print(w.pos_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-3, -2, -1, 0, 1, 2, 3}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(two_authors['f_reading'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('../output/features/features_biotech.pkl', 'rb') as file:\n",
    "    features = pickle.load(file)\n",
    "features.groupby('id')['spelling_errors'].count()\n",
    "data = features.loc[features['id'].isin([691951, 2373875])]\n",
    "\n",
    "two_authors = data.loc[data['f_reading']!= 0]\n",
    "two_authors['f_reading'].mask(two_authors['f_reading'] == 'very_easy', -3, inplace=True)\n",
    "two_authors['f_reading'].mask(two_authors['f_reading'] == 'easy', -2, inplace=True)\n",
    "two_authors['f_reading'].mask(two_authors['f_reading'] == 'fairly_easy', -1, inplace=True)\n",
    "two_authors['f_reading'].mask(two_authors['f_reading'] == 'standard', 0, inplace=True)\n",
    "two_authors['f_reading'].mask(two_authors['f_reading'] == 'difficult', 1, inplace=True)\n",
    "two_authors['f_reading'].mask(two_authors['f_reading'] == 'fairly_difficult', 2, inplace=True)\n",
    "two_authors['f_reading'].mask(two_authors['f_reading'] == 'very_confusing', 3, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221\n",
      "221\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(two_authors.isnull().sum().sum())\n",
    "print(two_authors['syllable_no'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_381/2170933704.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mrbf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rbf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mperm_importance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpermutation_importance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrbf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             X, y = self._validate_data(X, y, dtype=np.float64,\n\u001b[0m\u001b[1;32m    170\u001b[0m                                        \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                                        accept_large_sparse=False)\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[1;32m    872\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m             _assert_all_finite(array,\n\u001b[0m\u001b[1;32m    721\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m    102\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    104\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     (type_err,\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "import numpy as np\n",
    "from sklearn.inspection import permutation_importance\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = two_authors.drop(['id'], axis = 1).values.tolist()\n",
    "y = two_authors['id'].tolist()\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, train_size=0.80, test_size=0.20)\n",
    "\n",
    "\n",
    "rbf = svm.SVC(kernel='rbf', C=2).fit(X_train, y_train)\n",
    "perm_importance = permutation_importance(rbf, X_test, y_test)\n",
    "\n",
    "rbf_pred = rbf.predict(X_test)\n",
    "rbf_accuracy = accuracy_score(y_test, rbf_pred)\n",
    "rbf_f1 = f1_score(y_test, rbf_pred, average='weighted')\n",
    "print('Accuracy (RBF Kernel): ', \"%.2f\" % (rbf_accuracy*100))\n",
    "print('F1 (RBF Kernel): ', \"%.2f\" % (rbf_f1*100))\n",
    "\n",
    "feature_names = ['yule', 'fk_grade', 'f_reading', 'gunning_fog', 'honore_r', 'avg_word_length', 'syllable_no', 'spelling_errors', 'no_tag', 'sym', 'punct', 'mean_word_rank', 'of_freq', 'is_freq', 'the_freq', 'been_freq','nvn','nnv','vnn','vnv','nap','nnc','nad','dna','nnn','nan','vad']\n",
    "features = np.array(feature_names)\n",
    "\n",
    "sorted_idx = perm_importance.importances_mean.argsort()\n",
    "plt.barh(features[sorted_idx], perm_importance.importances_mean[sorted_idx])\n",
    "plt.xlabel(\"Permutation Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: Polynomial kernel\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      691951       0.81      0.84      0.82        50\n",
      "     2373875       0.83      0.79      0.81        48\n",
      "\n",
      "    accuracy                           0.82        98\n",
      "   macro avg       0.82      0.82      0.82        98\n",
      "weighted avg       0.82      0.82      0.82        98\n",
      "\n",
      "Evaluation: RBF kernel\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      691951       0.65      0.79      0.71        42\n",
      "     2373875       0.81      0.68      0.74        56\n",
      "\n",
      "    accuracy                           0.72        98\n",
      "   macro avg       0.73      0.73      0.72        98\n",
      "weighted avg       0.74      0.72      0.73        98\n",
      "\n",
      "Evaluation: Sigmoid kernel\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      691951       0.23      0.31      0.27        42\n",
      "     2373875       0.31      0.23      0.27        56\n",
      "\n",
      "    accuracy                           0.27        98\n",
      "   macro avg       0.27      0.27      0.27        98\n",
      "weighted avg       0.28      0.27      0.27        98\n",
      "\n",
      "Evaluation: Linear kernel\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      691951       0.85      0.76      0.80        37\n",
      "     2373875       0.86      0.92      0.89        61\n",
      "\n",
      "    accuracy                           0.86        98\n",
      "   macro avg       0.86      0.84      0.84        98\n",
      "weighted avg       0.86      0.86      0.86        98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kernels = ['Polynomial', 'RBF', 'Sigmoid','Linear']\n",
    "def getClassifier(ktype):\n",
    "    if ktype == 0:\n",
    "        # Polynomial kernal\n",
    "        return svm.SVC(kernel='poly', degree=8, gamma=\"auto\")\n",
    "    elif ktype == 1:\n",
    "        # Radial Basis Function kernal\n",
    "        return svm.SVC(kernel='rbf', gamma=\"auto\")\n",
    "    elif ktype == 2:\n",
    "        # Sigmoid kernal\n",
    "        return svm.SVC(kernel='sigmoid', gamma=\"auto\")\n",
    "    elif ktype == 3:\n",
    "        # Linear kernal\n",
    "        return svm.SVC(kernel='linear', gamma=\"auto\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.20)\n",
    "for i in range(4):    \n",
    "    svclassifier = getClassifier(i) \n",
    "    svclassifier.fit(X_train, y_train)# Make prediction\n",
    "    y_pred = svclassifier.predict(X_test)# Evaluate our model\n",
    "    print(\"Evaluation:\", kernels[i], \"kernel\")\n",
    "    print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   4.3s\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   3.7s\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   6.5s\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   3.9s\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   6.4s\n",
      "[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   0.4s\n",
      "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   0.5s\n",
      "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   0.3s\n",
      "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   0.3s\n",
      "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   0.3s\n",
      "[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END ....................C=0.1, gamma=0.001, kernel=poly; total time=   0.0s\n",
      "[CV] END ....................C=0.1, gamma=0.001, kernel=poly; total time=   0.0s\n",
      "[CV] END ....................C=0.1, gamma=0.001, kernel=poly; total time=   0.0s\n",
      "[CV] END ....................C=0.1, gamma=0.001, kernel=poly; total time=   0.0s\n",
      "[CV] END ....................C=0.1, gamma=0.001, kernel=poly; total time=   0.0s\n",
      "[CV] END .................C=0.1, gamma=0.001, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .................C=0.1, gamma=0.001, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .................C=0.1, gamma=0.001, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .................C=0.1, gamma=0.001, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .................C=0.1, gamma=0.001, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=  16.5s\n",
      "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=  58.4s\n",
      "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=  49.6s\n",
      "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=  15.3s\n",
      "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=  39.7s\n",
      "[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   0.7s\n",
      "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   0.5s\n",
      "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   0.8s\n",
      "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   1.5s\n",
      "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   0.8s\n",
      "[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=   0.1s\n",
      "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=   0.1s\n",
      "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=1, gamma=0.001, kernel=poly; total time=   0.0s\n",
      "[CV] END ......................C=1, gamma=0.001, kernel=poly; total time=   0.0s\n",
      "[CV] END ......................C=1, gamma=0.001, kernel=poly; total time=   0.0s\n",
      "[CV] END ......................C=1, gamma=0.001, kernel=poly; total time=   0.0s\n",
      "[CV] END ......................C=1, gamma=0.001, kernel=poly; total time=   0.0s\n",
      "[CV] END ...................C=1, gamma=0.001, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ...................C=1, gamma=0.001, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ...................C=1, gamma=0.001, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ...................C=1, gamma=0.001, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ...................C=1, gamma=0.001, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=10, gamma=1, kernel=poly; total time= 1.0min\n",
      "[CV] END .........................C=10, gamma=1, kernel=poly; total time= 1.7min\n",
      "[CV] END .........................C=10, gamma=1, kernel=poly; total time= 1.9min\n",
      "[CV] END .........................C=10, gamma=1, kernel=poly; total time=  14.7s\n",
      "[CV] END .........................C=10, gamma=1, kernel=poly; total time= 1.3min\n",
      "[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=   2.9s\n",
      "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=   1.2s\n",
      "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=   1.5s\n",
      "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=   5.4s\n",
      "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=   1.9s\n",
      "[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=   0.3s\n",
      "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=   0.1s\n",
      "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=   0.2s\n",
      "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=   0.1s\n",
      "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=   0.1s\n",
      "[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=10, gamma=0.001, kernel=poly; total time=   0.0s\n",
      "[CV] END .....................C=10, gamma=0.001, kernel=poly; total time=   0.0s\n",
      "[CV] END .....................C=10, gamma=0.001, kernel=poly; total time=   0.0s\n",
      "[CV] END .....................C=10, gamma=0.001, kernel=poly; total time=   0.0s\n",
      "[CV] END .....................C=10, gamma=0.001, kernel=poly; total time=   0.0s\n",
      "[CV] END ..................C=10, gamma=0.001, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..................C=10, gamma=0.001, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..................C=10, gamma=0.001, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..................C=10, gamma=0.001, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..................C=10, gamma=0.001, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=100, gamma=1, kernel=poly; total time= 1.0min\n",
      "[CV] END ........................C=100, gamma=1, kernel=poly; total time= 1.7min\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': [0.1,1, 10], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid']}\n",
    "grid = GridSearchCV(svm.SVC(), param_grid, refit=True, verbose=2)\n",
    "grid.fit(X_train,y_train)\n",
    "print(grid.best_estimator_)\n",
    "grid_predictions = grid.predict(X_test)\n",
    "print(classification_report(y_test,grid_predictions))#Output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2059d54db69b409a30853061144d64e50a683fe5528e4a38afc9635b706f8a27"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
